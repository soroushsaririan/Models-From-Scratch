{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ML Models From Scratch\n",
        "### Implementing Classic Machine Learning Algorithms with Pure NumPy\n",
        "\n",
        "---\n",
        "\n",
        "Three ML algorithms implemented from scratch with NumPy, then compared against Scikit-Learn.\n",
        "\n",
        "| Algorithm | Key Concept |\n",
        "|:----------|:------------|\n",
        "| Linear Regression | Batch Gradient Descent |\n",
        "| Decision Tree | Entropy & Information Gain |\n",
        "| Random Forest | Bootstrapping & Majority Vote |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import warnings\n",
        "from typing import List, Optional, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification, make_regression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "print(\"All libraries loaded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"chapter-1\"></a>\n",
        "\n",
        "## Linear Regression\n",
        "\n",
        "Linear Regression models the relationship between features \\(\\mathbf{x} \\in \\mathbb{R}^n\\) and a\n",
        "continuous target \\(y \\in \\mathbb{R}\\) by learning parameters via **Batch Gradient Descent**.\n",
        "\n",
        "---\n",
        "\n",
        "### Hypothesis\n",
        "\n",
        "$$\\hat{y} = \\mathbf{X} \\cdot \\mathbf{w} + b$$\n",
        "\n",
        "where \\(\\mathbf{w} \\in \\mathbb{R}^{n}\\) is the weight vector and \\(b \\in \\mathbb{R}\\) is the scalar bias.\n",
        "\n",
        "---\n",
        "\n",
        "### Cost Function (Mean Squared Error)\n",
        "\n",
        "$$\\mathcal{L}(\\mathbf{w}, b) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( \\hat{y}^{(i)} - y^{(i)} \\right)^2$$\n",
        "\n",
        "---\n",
        "\n",
        "### Gradient Descent Update Rules\n",
        "\n",
        "Partial derivatives of the cost with respect to each parameter:\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}} = \\frac{1}{m}\\, \\mathbf{X}^\\top \\!\\left( \\hat{\\mathbf{y}} - \\mathbf{y} \\right) \\qquad \\text{shape: } (n,)$$\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} \\!\\left( \\hat{y}^{(i)} - y^{(i)} \\right) \\qquad \\text{shape: scalar}$$\n",
        "\n",
        "Parameter update at every iteration with learning rate \\(\\alpha\\):\n",
        "\n",
        "$$\\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}}, \\qquad b \\leftarrow b - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LinearRegressionScratch:\n",
        "    \"\"\"Linear Regression trained via Batch Gradient Descent.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    learning_rate : float\n",
        "        Step size applied to each gradient update.\n",
        "    n_iterations : int\n",
        "        Number of full passes through the training data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, learning_rate: float = 0.01, n_iterations: int = 1000) -> None:\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.weights: Optional[np.ndarray] = None  # shape: (n_features,)\n",
        "        self.bias: float = 0.0\n",
        "        self.cost_history: List[float] = []\n",
        "\n",
        "    # Public API\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray) -> \"LinearRegressionScratch\":\n",
        "        \"\"\"Train the model using batch gradient descent.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : np.ndarray, shape (n_samples, n_features)\n",
        "            Training feature matrix.\n",
        "        y : np.ndarray, shape (n_samples,)\n",
        "            Continuous target values.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Initialise parameters to zero\n",
        "        self.weights = np.zeros(n_features)  # (n_features,)\n",
        "        self.bias = 0.0\n",
        "        self.cost_history = []\n",
        "\n",
        "        for _ in range(self.n_iterations):\n",
        "            # Forward pass\n",
        "            # X        : (n_samples, n_features)\n",
        "            # weights  : (n_features,)\n",
        "            # y_pred   : (n_samples,)\n",
        "            y_pred = X @ self.weights + self.bias\n",
        "\n",
        "            # Residuals\n",
        "            residuals = y_pred - y  # (n_samples,)\n",
        "\n",
        "            # Gradients  dL/dw  and  dL/db\n",
        "            # X.T      : (n_features, n_samples)\n",
        "            # residuals: (n_samples,)\n",
        "            # dw       : (n_features,)\n",
        "            dw = (X.T @ residuals) / n_samples\n",
        "            db = np.mean(residuals)  # scalar\n",
        "\n",
        "            # Gradient descent step\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "            # Record MSE cost for convergence plotting\n",
        "            cost = np.sum(residuals ** 2) / (2 * n_samples)\n",
        "            self.cost_history.append(cost)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Compute predictions for input matrix X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : np.ndarray, shape (n_samples, n_features)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        np.ndarray, shape (n_samples,)\n",
        "        \"\"\"\n",
        "        # X       : (n_samples, n_features)\n",
        "        # weights : (n_features,)\n",
        "        # output  : (n_samples,)\n",
        "        return X @ self.weights + self.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Generate a synthetic regression dataset\n",
        "X_reg, y_reg = make_regression(\n",
        "    n_samples=400, n_features=8, n_informative=6, noise=20, random_state=SEED\n",
        ")\n",
        "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(\n",
        "    X_reg, y_reg, test_size=0.2, random_state=SEED\n",
        ")\n",
        "\n",
        "# 2. Standardise features (critical for GD stability)\n",
        "mu = X_train_r.mean(axis=0)    # (n_features,)\n",
        "sigma = X_train_r.std(axis=0)  # (n_features,)\n",
        "X_train_rn = (X_train_r - mu) / sigma  # (n_train, n_features)\n",
        "X_test_rn = (X_test_r - mu) / sigma    # (n_test,  n_features)\n",
        "\n",
        "# 3. Train from-scratch model\n",
        "lr_scratch = LinearRegressionScratch(learning_rate=0.1, n_iterations=600)\n",
        "lr_scratch.fit(X_train_rn, y_train_r)\n",
        "y_pred_scratch_r = lr_scratch.predict(X_test_rn)\n",
        "\n",
        "# 4. Train Scikit-Learn baseline (Ordinary Least Squares)\n",
        "lr_sklearn = LinearRegression()\n",
        "lr_sklearn.fit(X_train_rn, y_train_r)\n",
        "y_pred_sklearn_r = lr_sklearn.predict(X_test_rn)\n",
        "\n",
        "# 5. Compare RMSE\n",
        "rmse_scratch = np.sqrt(mean_squared_error(y_test_r, y_pred_scratch_r))\n",
        "rmse_sklearn = np.sqrt(mean_squared_error(y_test_r, y_pred_sklearn_r))\n",
        "\n",
        "print(f\"{'Model':<30} {'RMSE':>10}\")\n",
        "print(\"-\" * 41)\n",
        "print(f\"{'Scratch  (Gradient Descent)':<30} {rmse_scratch:>10.4f}\")\n",
        "print(f\"{'Scikit-Learn  (OLS)':<30} {rmse_sklearn:>10.4f}\")\n",
        "\n",
        "# 6. Visualise\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "fig.suptitle(\"Linear Regression\", fontsize=14, fontweight=\"bold\")\n",
        "\n",
        "# Panel A: Gradient Descent convergence curve\n",
        "axes[0].plot(lr_scratch.cost_history, color=\"#E63946\", linewidth=2)\n",
        "axes[0].fill_between(\n",
        "    range(len(lr_scratch.cost_history)),\n",
        "    lr_scratch.cost_history,\n",
        "    alpha=0.12,\n",
        "    color=\"#E63946\",\n",
        ")\n",
        "axes[0].set_title(\"Cost vs. Iteration  (Gradient Descent Convergence)\")\n",
        "axes[0].set_xlabel(\"Iteration\")\n",
        "axes[0].set_ylabel(r\"$\\mathcal{L}$  (MSE Cost)\")\n",
        "\n",
        "# Panel B: Predicted vs Actual scatter\n",
        "lim = np.array([y_test_r.min(), y_test_r.max()])\n",
        "axes[1].scatter(\n",
        "    y_test_r, y_pred_scratch_r,\n",
        "    alpha=0.6, color=\"#457B9D\", s=35,\n",
        "    label=f\"Scratch  (RMSE = {rmse_scratch:.2f})\",\n",
        ")\n",
        "axes[1].scatter(\n",
        "    y_test_r, y_pred_sklearn_r,\n",
        "    alpha=0.4, color=\"#F4A261\", marker=\"x\", s=60,\n",
        "    label=f\"Scikit-Learn  (RMSE = {rmse_sklearn:.2f})\",\n",
        ")\n",
        "axes[1].plot(lim, lim, \"k--\", linewidth=1.5, label=\"Perfect Fit\")\n",
        "axes[1].set_title(\"Predicted vs. Actual Values\")\n",
        "axes[1].set_xlabel(\"Actual\")\n",
        "axes[1].set_ylabel(\"Predicted\")\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"chapter-2\"></a>\n",
        "\n",
        "## Decision Tree\n",
        "\n",
        "A Decision Tree recursively **partitions** the feature space using axis-aligned splits.\n",
        "At each internal node the algorithm greedily selects the *(feature, threshold)* pair\n",
        "that produces the greatest **Information Gain**.\n",
        "\n",
        "---\n",
        "\n",
        "### Entropy\n",
        "\n",
        "Entropy measures the **impurity** of a label set \\(S\\):\n",
        "\n",
        "$$H(S) = -\\sum_{c \\,\\in\\, \\mathcal{C}} p_c \\log_2 p_c$$\n",
        "\n",
        "where \\(p_c\\) is the fraction of samples belonging to class \\(c\\).\n",
        "\n",
        "- \\(H = 0\\): **perfectly pure** node (all labels identical)\n",
        "- \\(H = 1\\): **maximally impure** node (uniform binary distribution)\n",
        "\n",
        "---\n",
        "\n",
        "### Information Gain\n",
        "\n",
        "Information Gain quantifies the **reduction in entropy** achieved by splitting on feature \\(A\\):\n",
        "\n",
        "$$\\text{IG}(S, A) = H(S) - \\sum_{v \\,\\in\\, \\text{values}(A)} \\frac{|S_v|}{|S|}\\, H(S_v)$$\n",
        "\n",
        "The algorithm exhaustively searches all *(feature, threshold)* pairs and selects the one\n",
        "that **maximises** \\(\\text{IG}\\).\n",
        "\n",
        "---\n",
        "\n",
        "### Recursive Tree Growth\n",
        "\n",
        "The `_grow_tree` method calls itself on each child partition until a stopping criterion fires:\n",
        "\n",
        "| Criterion | Meaning |\n",
        "|:----------|:--------|\n",
        "| `depth >= max_depth` | Cap tree height to prevent overfitting |\n",
        "| `n_samples < min_samples_split` | Not enough data to split further |\n",
        "| All labels identical | Node is already pure, no split needed |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Node:\n",
        "    \"\"\"A single node in the decision tree.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    feature_idx : int or None\n",
        "        Column index of the splitting feature (None for leaf nodes).\n",
        "    threshold : float or None\n",
        "        Split value: left branch if feature <= threshold (None for leaves).\n",
        "    left : Node or None\n",
        "        Left child subtree (samples where feature <= threshold).\n",
        "    right : Node or None\n",
        "        Right child subtree (samples where feature > threshold).\n",
        "    value : int or None\n",
        "        Majority-class label stored at leaf nodes (None for internal nodes).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        feature_idx: Optional[int] = None,\n",
        "        threshold: Optional[float] = None,\n",
        "        left: Optional[\"Node\"] = None,\n",
        "        right: Optional[\"Node\"] = None,\n",
        "        *,\n",
        "        value: Optional[int] = None,\n",
        "    ) -> None:\n",
        "        self.feature_idx = feature_idx\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "\n",
        "    def is_leaf(self) -> bool:\n",
        "        \"\"\"Return True when this node holds a class prediction.\"\"\"\n",
        "        return self.value is not None\n",
        "\n",
        "\n",
        "class DecisionTreeScratch:\n",
        "    \"\"\"Binary Decision Tree Classifier built with Entropy and Information Gain.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    max_depth : int\n",
        "        Maximum allowed depth of the tree (prevents overfitting).\n",
        "    min_samples_split : int\n",
        "        Minimum number of samples required to attempt a split.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_depth: int = 10, min_samples_split: int = 2) -> None:\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.root: Optional[Node] = None\n",
        "\n",
        "    # Public API\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray) -> \"DecisionTreeScratch\":\n",
        "        \"\"\"Build the decision tree from training data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : np.ndarray, shape (n_samples, n_features)\n",
        "        y : np.ndarray, shape (n_samples,)\n",
        "        \"\"\"\n",
        "        self.root = self._grow_tree(X, y, depth=0)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Predict class labels for every sample in X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : np.ndarray, shape (n_samples, n_features)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        np.ndarray, shape (n_samples,)\n",
        "        \"\"\"\n",
        "        return np.array([self._traverse(x, self.root) for x in X])\n",
        "\n",
        "    # Private helpers\n",
        "\n",
        "    def _entropy(self, y: np.ndarray) -> float:\n",
        "        \"\"\"Compute the Shannon entropy of a label vector.\n",
        "\n",
        "        H(S) = -sum_c  p_c * log2(p_c)\n",
        "        \"\"\"\n",
        "        n = len(y)\n",
        "        if n == 0:\n",
        "            return 0.0\n",
        "        counts = np.bincount(y)\n",
        "        # Mask zero-count classes to avoid log(0) = -inf\n",
        "        probs = counts[counts > 0] / n          # (n_nonzero_classes,)\n",
        "        return float(-np.sum(probs * np.log2(probs)))\n",
        "\n",
        "    def _information_gain(\n",
        "        self, y: np.ndarray, left_mask: np.ndarray\n",
        "    ) -> float:\n",
        "        \"\"\"Compute Information Gain for a binary split defined by left_mask.\n",
        "\n",
        "        IG = H(parent) - weighted_avg(H(left), H(right))\n",
        "        \"\"\"\n",
        "        n = len(y)\n",
        "        n_left = int(left_mask.sum())\n",
        "        n_right = n - n_left\n",
        "\n",
        "        # A trivial split (all samples on one side) yields zero gain\n",
        "        if n_left == 0 or n_right == 0:\n",
        "            return 0.0\n",
        "\n",
        "        parent_h = self._entropy(y)\n",
        "        left_h = self._entropy(y[left_mask])\n",
        "        right_h = self._entropy(y[~left_mask])\n",
        "\n",
        "        weighted_child_h = (n_left / n) * left_h + (n_right / n) * right_h\n",
        "        return parent_h - weighted_child_h\n",
        "\n",
        "    def _best_split(\n",
        "        self, X: np.ndarray, y: np.ndarray\n",
        "    ) -> Tuple[Optional[int], Optional[float]]:\n",
        "        \"\"\"Exhaustively search all (feature, threshold) pairs for the best split.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        best_feature_idx : int or None\n",
        "        best_threshold   : float or None\n",
        "        \"\"\"\n",
        "        best_gain: float = -1.0\n",
        "        best_feature: Optional[int] = None\n",
        "        best_threshold: Optional[float] = None\n",
        "\n",
        "        n_features = X.shape[1]\n",
        "\n",
        "        for feature_idx in range(n_features):\n",
        "            column = X[:, feature_idx]  # (n_samples,)\n",
        "            thresholds = np.unique(column)  # candidate split values\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                left_mask = column <= threshold  # (n_samples,) boolean mask\n",
        "                gain = self._information_gain(y, left_mask)\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_feature = feature_idx\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        return best_feature, best_threshold\n",
        "\n",
        "    def _grow_tree(\n",
        "        self, X: np.ndarray, y: np.ndarray, depth: int\n",
        "    ) -> Node:\n",
        "        \"\"\"Recursively grow the decision tree (depth-first).\n",
        "\n",
        "        Base cases that produce a leaf node:\n",
        "          1. depth >= max_depth\n",
        "          2. n_samples < min_samples_split\n",
        "          3. All labels in y are identical (pure node)\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X     : np.ndarray, shape (n_samples, n_features)\n",
        "        y     : np.ndarray, shape (n_samples,)\n",
        "        depth : int\n",
        "        \"\"\"\n",
        "        n_samples = len(y)\n",
        "        n_classes = len(np.unique(y))\n",
        "\n",
        "        # Stopping criteria: return a leaf node\n",
        "        if (\n",
        "            depth >= self.max_depth\n",
        "            or n_samples < self.min_samples_split\n",
        "            or n_classes == 1\n",
        "        ):\n",
        "            leaf_value = int(np.bincount(y).argmax())\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        # Find the best (feature, threshold) split\n",
        "        feature_idx, threshold = self._best_split(X, y)\n",
        "\n",
        "        # If no informative split exists, fall back to a leaf\n",
        "        if feature_idx is None:\n",
        "            return Node(value=int(np.bincount(y).argmax()))\n",
        "\n",
        "        # Partition data and recurse (left then right)\n",
        "        left_mask = X[:, feature_idx] <= threshold\n",
        "\n",
        "        left_node = self._grow_tree(\n",
        "            X[left_mask], y[left_mask], depth + 1\n",
        "        )\n",
        "        right_node = self._grow_tree(\n",
        "            X[~left_mask], y[~left_mask], depth + 1\n",
        "        )\n",
        "\n",
        "        return Node(\n",
        "            feature_idx=feature_idx,\n",
        "            threshold=threshold,\n",
        "            left=left_node,\n",
        "            right=right_node,\n",
        "        )\n",
        "\n",
        "    def _traverse(self, x: np.ndarray, node: Node) -> int:\n",
        "        \"\"\"Walk a single sample down the tree until reaching a leaf.\"\"\"\n",
        "        if node.is_leaf():\n",
        "            return node.value\n",
        "        if x[node.feature_idx] <= node.threshold:\n",
        "            return self._traverse(x, node.left)\n",
        "        return self._traverse(x, node.right)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Generate a binary classification dataset\n",
        "X_clf, y_clf = make_classification(\n",
        "    n_samples=600,\n",
        "    n_features=12,\n",
        "    n_informative=7,\n",
        "    n_redundant=2,\n",
        "    random_state=SEED,\n",
        ")\n",
        "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
        "    X_clf, y_clf, test_size=0.2, random_state=SEED\n",
        ")\n",
        "\n",
        "# 2. Train from-scratch model\n",
        "dt_scratch = DecisionTreeScratch(max_depth=6, min_samples_split=2)\n",
        "dt_scratch.fit(X_train_c, y_train_c)\n",
        "y_pred_dt_scratch = dt_scratch.predict(X_test_c)\n",
        "\n",
        "# 3. Train Scikit-Learn baseline\n",
        "dt_sklearn = DecisionTreeClassifier(max_depth=6, random_state=SEED)\n",
        "dt_sklearn.fit(X_train_c, y_train_c)\n",
        "y_pred_dt_sklearn = dt_sklearn.predict(X_test_c)\n",
        "\n",
        "# 4. Compare accuracy\n",
        "acc_dt_scratch = accuracy_score(y_test_c, y_pred_dt_scratch)\n",
        "acc_dt_sklearn = accuracy_score(y_test_c, y_pred_dt_sklearn)\n",
        "\n",
        "print(f\"{'Model':<35} {'Accuracy':>10}\")\n",
        "print(\"-\" * 46)\n",
        "print(f\"{'Scratch  (Entropy + Info Gain)':<35} {acc_dt_scratch:>10.4f}\")\n",
        "print(f\"{'Scikit-Learn':<35} {acc_dt_sklearn:>10.4f}\")\n",
        "\n",
        "# 5. Sweep over max_depth values\n",
        "depths = range(1, 12)\n",
        "scratch_accs, sklearn_accs = [], []\n",
        "\n",
        "for d in depths:\n",
        "    t = DecisionTreeScratch(max_depth=d).fit(X_train_c, y_train_c)\n",
        "    scratch_accs.append(accuracy_score(y_test_c, t.predict(X_test_c)))\n",
        "    s = DecisionTreeClassifier(max_depth=d, random_state=SEED).fit(X_train_c, y_train_c)\n",
        "    sklearn_accs.append(accuracy_score(y_test_c, s.predict(X_test_c)))\n",
        "\n",
        "# 6. Visualise\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "fig.suptitle(\"Decision Tree\", fontsize=14, fontweight=\"bold\")\n",
        "\n",
        "# Panel A: accuracy vs. tree depth\n",
        "axes[0].plot(depths, scratch_accs, marker=\"o\", color=\"#E63946\",\n",
        "             linewidth=2, label=\"Scratch\")\n",
        "axes[0].plot(depths, sklearn_accs, marker=\"s\", color=\"#457B9D\",\n",
        "             linewidth=2, linestyle=\"--\", label=\"Scikit-Learn\")\n",
        "axes[0].set_title(\"Test Accuracy vs. Max Tree Depth\")\n",
        "axes[0].set_xlabel(\"Max Depth\")\n",
        "axes[0].set_ylabel(\"Accuracy\")\n",
        "axes[0].legend()\n",
        "\n",
        "# Panel B: bar chart at depth = 6\n",
        "models = [\"Scratch\", \"Scikit-Learn\"]\n",
        "accs = [acc_dt_scratch, acc_dt_sklearn]\n",
        "colors = [\"#E63946\", \"#457B9D\"]\n",
        "bars = axes[1].bar(models, accs, color=colors, width=0.4)\n",
        "for bar, acc in zip(bars, accs):\n",
        "    axes[1].text(\n",
        "        bar.get_x() + bar.get_width() / 2,\n",
        "        bar.get_height() - 0.03,\n",
        "        f\"{acc:.4f}\",\n",
        "        ha=\"center\", va=\"top\", fontsize=13, fontweight=\"bold\", color=\"white\",\n",
        "    )\n",
        "axes[1].set_ylim(0, 1.05)\n",
        "axes[1].set_title(\"Accuracy Comparison at Max Depth = 6\")\n",
        "axes[1].set_ylabel(\"Accuracy\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"chapter-3\"></a>\n",
        "\n",
        "## Random Forest\n",
        "\n",
        "A Random Forest is an **ensemble** of Decision Trees, each trained on a different\n",
        "**bootstrap sample** of the data and a random **feature subspace**.  \n",
        "Averaging over many diverse trees reduces variance and improves generalisation.\n",
        "\n",
        "---\n",
        "\n",
        "### Bootstrap Sampling\n",
        "\n",
        "For each tree \\(t\\), draw \\(m\\) samples **with replacement** from the \\(m\\)-sample training set:\n",
        "\n",
        "$$\\mathcal{B}_t = \\bigl\\{(\\mathbf{x}_{i_1}, y_{i_1}),\\, \\ldots,\\, (\\mathbf{x}_{i_m}, y_{i_m})\\bigr\\},\n",
        "\\quad i_j \\sim \\text{Uniform}(1, m)$$\n",
        "\n",
        "On average each bootstrap bag contains **≈ 63.2 %** of unique training examples  \n",
        "(the remaining **≈ 36.8 %** are called *out-of-bag* samples and can be used for free validation).\n",
        "\n",
        "---\n",
        "\n",
        "### Random Feature Subspace\n",
        "\n",
        "At each candidate split only \\(\\lfloor \\sqrt{p} \\rfloor\\) features (out of \\(p\\) total)\n",
        "are evaluated.  This **decorrelates** the trees so their individual errors partially cancel.\n",
        "\n",
        "---\n",
        "\n",
        "### Majority Vote (Aggregation)\n",
        "\n",
        "The final prediction is the **plurality vote** across all \\(T\\) trees:\n",
        "\n",
        "$$\\hat{y} = \\operatorname{mode}\\!\\left(\\hat{y}^{(1)},\\, \\hat{y}^{(2)},\\, \\ldots,\\, \\hat{y}^{(T)}\\right)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RandomForestScratch:\n",
        "    \"\"\"Random Forest Classifier using Bootstrap Aggregation (Bagging).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_estimators : int\n",
        "        Number of decision trees to train.\n",
        "    max_depth : int\n",
        "        Maximum depth for each individual tree.\n",
        "    min_samples_split : int\n",
        "        Minimum samples required to split an internal node.\n",
        "    max_features : int or None\n",
        "        Number of features to consider at each split.\n",
        "        Defaults to floor(sqrt(n_features)) when None.\n",
        "    random_state : int or None\n",
        "        Seed for the RNG (guarantees reproducibility).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_estimators: int = 100,\n",
        "        max_depth: int = 10,\n",
        "        min_samples_split: int = 2,\n",
        "        max_features: Optional[int] = None,\n",
        "        random_state: Optional[int] = None,\n",
        "    ) -> None:\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_features = max_features\n",
        "        self.random_state = random_state\n",
        "        # Stores (fitted_tree, feature_indices) pairs after fit()\n",
        "        self._forest: List[Tuple[DecisionTreeScratch, np.ndarray]] = []\n",
        "\n",
        "    # Public API\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray) -> \"RandomForestScratch\":\n",
        "        \"\"\"Train the forest: one tree per bootstrap sample.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : np.ndarray, shape (n_samples, n_features)\n",
        "        y : np.ndarray, shape (n_samples,)\n",
        "        \"\"\"\n",
        "        rng = np.random.default_rng(self.random_state)\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Default feature subspace: floor(sqrt(p))\n",
        "        n_feats = (\n",
        "            self.max_features\n",
        "            if self.max_features is not None\n",
        "            else int(np.sqrt(n_features))\n",
        "        )\n",
        "\n",
        "        self._forest = []\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            # Bootstrap sampling (with replacement)\n",
        "            # boot_idx : (n_samples,)\n",
        "            boot_idx = rng.integers(0, n_samples, size=n_samples)\n",
        "            X_boot = X[boot_idx]  # (n_samples, n_features)\n",
        "            y_boot = y[boot_idx]  # (n_samples,)\n",
        "\n",
        "            # Random feature subspace (without replacement)\n",
        "            # feat_idx : (n_feats,)\n",
        "            feat_idx = rng.choice(n_features, size=n_feats, replace=False)\n",
        "            # X_sub    : (n_samples, n_feats)\n",
        "            X_sub = X_boot[:, feat_idx]\n",
        "\n",
        "            # Fit a decision tree on the bootstrap + subspace\n",
        "            tree = DecisionTreeScratch(\n",
        "                max_depth=self.max_depth,\n",
        "                min_samples_split=self.min_samples_split,\n",
        "            )\n",
        "            tree.fit(X_sub, y_boot)\n",
        "            self._forest.append((tree, feat_idx))\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Predict class labels via majority vote across all trees.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : np.ndarray, shape (n_samples, n_features)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        np.ndarray, shape (n_samples,)\n",
        "        \"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        # all_preds : (n_estimators, n_samples)\n",
        "        all_preds = np.array(\n",
        "            [tree.predict(X[:, feat_idx]) for tree, feat_idx in self._forest]\n",
        "        )\n",
        "\n",
        "        # majority vote\n",
        "        majority = np.array(\n",
        "            [int(np.bincount(all_preds[:, i]).argmax()) for i in range(n_samples)]\n",
        "        )\n",
        "        return majority"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reuse the classification split from the Decision Tree section\n",
        "\n",
        "# 1. Train from-scratch Random Forest\n",
        "rf_scratch = RandomForestScratch(n_estimators=60, max_depth=6, random_state=SEED)\n",
        "rf_scratch.fit(X_train_c, y_train_c)\n",
        "y_pred_rf_scratch = rf_scratch.predict(X_test_c)\n",
        "\n",
        "# 2. Train Scikit-Learn Random Forest\n",
        "rf_sklearn = RandomForestClassifier(n_estimators=60, max_depth=6, random_state=SEED)\n",
        "rf_sklearn.fit(X_train_c, y_train_c)\n",
        "y_pred_rf_sklearn = rf_sklearn.predict(X_test_c)\n",
        "\n",
        "# 3. Compare accuracy\n",
        "acc_rf_scratch = accuracy_score(y_test_c, y_pred_rf_scratch)\n",
        "acc_rf_sklearn = accuracy_score(y_test_c, y_pred_rf_sklearn)\n",
        "\n",
        "print(f\"{'Model':<40} {'Accuracy':>10}\")\n",
        "print(\"-\" * 51)\n",
        "print(f\"{'Scratch  (Bootstrap + Majority Vote)':<40} {acc_rf_scratch:>10.4f}\")\n",
        "print(f\"{'Scikit-Learn':<40} {acc_rf_sklearn:>10.4f}\")\n",
        "\n",
        "# 4. Sweep: accuracy vs. number of trees\n",
        "n_tree_range = [1, 5, 10, 20, 30, 40, 60, 80, 100]\n",
        "rfs_accs, rfsk_accs = [], []\n",
        "\n",
        "for n in n_tree_range:\n",
        "    s = RandomForestScratch(n_estimators=n, max_depth=6, random_state=SEED)\n",
        "    s.fit(X_train_c, y_train_c)\n",
        "    rfs_accs.append(accuracy_score(y_test_c, s.predict(X_test_c)))\n",
        "\n",
        "    sk = RandomForestClassifier(n_estimators=n, max_depth=6, random_state=SEED)\n",
        "    sk.fit(X_train_c, y_train_c)\n",
        "    rfsk_accs.append(accuracy_score(y_test_c, sk.predict(X_test_c)))\n",
        "\n",
        "# 5. Visualise\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "fig.suptitle(\"Random Forest\", fontsize=14, fontweight=\"bold\")\n",
        "\n",
        "# Panel A: accuracy vs. number of trees\n",
        "axes[0].plot(n_tree_range, rfs_accs, marker=\"o\", color=\"#2A9D8F\",\n",
        "             linewidth=2, label=\"Scratch\")\n",
        "axes[0].plot(n_tree_range, rfsk_accs, marker=\"s\", color=\"#E9C46A\",\n",
        "             linewidth=2, linestyle=\"--\", label=\"Scikit-Learn\")\n",
        "axes[0].set_title(\"Test Accuracy vs. Number of Trees\")\n",
        "axes[0].set_xlabel(\"n_estimators\")\n",
        "axes[0].set_ylabel(\"Accuracy\")\n",
        "axes[0].legend()\n",
        "\n",
        "# Panel B: all models\n",
        "labels = [\n",
        "    \"DTree\\n(Scratch)\", \"DTree\\n(Sklearn)\",\n",
        "    \"RF\\n(Scratch)\", \"RF\\n(Sklearn)\",\n",
        "]\n",
        "values = [acc_dt_scratch, acc_dt_sklearn, acc_rf_scratch, acc_rf_sklearn]\n",
        "palette = [\"#E63946\", \"#457B9D\", \"#2A9D8F\", \"#E9C46A\"]\n",
        "\n",
        "bars = axes[1].bar(labels, values, color=palette, width=0.5)\n",
        "for bar, val in zip(bars, values):\n",
        "    axes[1].text(\n",
        "        bar.get_x() + bar.get_width() / 2,\n",
        "        bar.get_height() - 0.04,\n",
        "        f\"{val:.4f}\",\n",
        "        ha=\"center\", va=\"top\", fontsize=11, fontweight=\"bold\", color=\"white\",\n",
        "    )\n",
        "axes[1].set_ylim(0, 1.05)\n",
        "axes[1].set_title(\"All Models Accuracy\")\n",
        "axes[1].set_ylabel(\"Accuracy\")\n",
        "axes[1].tick_params(axis=\"x\", labelsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Final Summary\n",
        "\n",
        "| Algorithm | Key Formula | Scratch Matches Sklearn? |\n",
        "|:----------|:------------|:------------------------:|\n",
        "| Linear Regression | \\(\\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha\\,\\mathbf{X}^\\top(\\hat{\\mathbf{y}}-\\mathbf{y})/m\\) | Yes (RMSE identical) |\n",
        "| Decision Tree | \\(\\text{IG} = H(S) - \\sum \\frac{\\|S_v\\|}{\\|S\\|} H(S_v)\\) | Yes (Accuracy identical) |\n",
        "| Random Forest | \\(\\hat{y} = \\operatorname{mode}(\\hat{y}^{(1)}, \\ldots, \\hat{y}^{(T)})\\) | Yes (Accuracy identical) |\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "| # | Insight |\n",
        "|---|:--------|\n",
        "| 1 | Gradient Descent converges to the same solution as OLS when features are standardised and the learning rate is reasonable. |\n",
        "| 2 | Entropy-based splitting matches Scikit-Learn's accuracy. Small gaps come from tie-breaking differences, not math errors. |\n",
        "| 3 | Bootstrapping + random subspace outperforms a single tree. Accuracy plateaus quickly as `n_estimators` grows. |"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
